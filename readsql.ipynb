{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc5e5c3-c31e-48b7-865d-b1010917b6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "# 打开数据库连接\n",
    "db = pymysql.connect(host='bi-mysql-hrbidata-write.vip.sankuai.com',\n",
    "                     port=5002,\n",
    "                     user='Tableau',\n",
    "                     password='Tableau@2021',\n",
    "                     database='hrbidata')\n",
    "cursor = db.cursor()\n",
    "# sql2 = \"SELECT * from dim_posts_info\"\n",
    "sql2 = \"\"\"\n",
    "SELECT * \n",
    "FROM dim_posts_info\n",
    "WHERE createor_time >= DATE_SUB(CURDATE(), INTERVAL 7 DAY)\n",
    "AND createor_time < CURDATE()\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(sql2)\n",
    "\n",
    "results = cursor.fetchall()\n",
    "\n",
    "# # # 打印查询结果\n",
    "# for row in results:\n",
    "#     print(row)\n",
    "# cursor.close()\n",
    "\n",
    "# 获取列名\n",
    "columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "# 将结果转为DataFrame\n",
    "df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# 将DataFrame保存为CSV文件\n",
    "df.to_csv('result1/2024_results.csv', index=False, encoding='utf-8')\n",
    "\n",
    "# 创建帖子名称和内容的格式\n",
    "with open('result1/posts.txt', 'w', encoding='utf-8') as f:\n",
    "    for index, row in df.iterrows():\n",
    "        line = f\"帖子名称:{row['posts_title']}, content:{row['posts_digest']}\\n\"\n",
    "        f.write(line)\n",
    "\n",
    "# 关闭数据库连接\n",
    "cursor.close()\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a96925-c830-4eac-aeed-364706161f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改read_excel函数以读取特定的sheetpip\n",
    "app_id = \"1681124596859981898\" \n",
    "def read_excel(file_path, sheet_name):\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    return df\n",
    "    # return df.iloc[:100]\n",
    "\n",
    "\n",
    "def read_prompt_from_file(prompt_file_path):\n",
    "    with open(prompt_file_path, 'r', encoding='utf-8') as file:\n",
    "        prompt = file.read()\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# 发送模型推理请求\n",
    "def send_inference_request(prompt, app_id):\n",
    "    url = \"https://aigc.sankuai.com/v1/host-model/sankuai/inference\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json;charset=UTF-8\",\n",
    "        \"Authorization\": f\"Bearer {app_id}\"\n",
    "    }\n",
    "    data = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"model\": \"LongCat-Prime-8K-Chat\",  # 选择LongCat模型\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 2048,\n",
    "        \"top_p\": 1,\n",
    "        \"beam_width\": 4\n",
    "    }\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "        end_time = time.time()\n",
    "    except SSLError as e:\n",
    "        logging.error(f\"SSL Error occurred: {e}\")\n",
    "        raise Exception(\"SSL验证失败，请检查SSL证书。\") from e\n",
    "\n",
    "    duration = end_time - start_time\n",
    "    response_json = response.json()\n",
    "    # print(response.status_code)\n",
    "    print(response_json)  # 打印每次返回的结果\n",
    "    print(f\"Response Time: {duration:.2f} seconds\")  # 打印每次请求的所用时长\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        response_json = response.json()\n",
    "        print(response_json)\n",
    "        return response_json, duration\n",
    "    else:\n",
    "        print(f\"Request failed with status code: {response.status_code}\")\n",
    "        return None, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba5f5f1-ac87-4ee0-9166-972bc9275081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_inference(file_path, app_id, result_file, prompt_file_path, output_excel_path, logger, max_retries=10):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    prompt_template = read_prompt_from_file(prompt_file_path)\n",
    "    # inference_result, duration = send_inference_request(prompt_template, app_id)\n",
    "    # print(inference_result)\n",
    "    results = []\n",
    "    output_data = []\n",
    "    durations = []\n",
    "\n",
    "    # prompt = prompt_template.replace(\"##话题标题和话题内容##\", content)\n",
    "\n",
    "    # print(prompt)\n",
    "\n",
    "    # inference_result, duration = send_inference_request(prompt, app_id)\n",
    "\n",
    "    # print(inference_result)\n",
    "    # result_data = inference_result['data']['result']\n",
    "    # print(result_data)\n",
    "\n",
    "    from agent.pws import PWSBase\n",
    "    from llm_config import CHAT_LLM_MODE, CHAT_LLM\n",
    "\n",
    "    method = PWSBase(planner_model=CHAT_LLM_MODE[CHAT_LLM], solver_model=CHAT_LLM_MODE[CHAT_LLM])\n",
    "\n",
    "    for line in content.splitlines():\n",
    "        # print(line) 根据每一行内容生成一个推理请求，然后处理响应并记录结果。\n",
    "        prompt = prompt_template.replace(\"##话题标题和话题内容##\", line)\n",
    "        retries = 0\n",
    "        while retries < max_retries:\n",
    "            try:\n",
    "                # inference_result, duration = send_inference_request(prompt, app_id)\n",
    "                inference_result = method.run(prompt)\n",
    "                # durations.append(duration)\n",
    "                if inference_result is not None:\n",
    "                    try:\n",
    "                        # result_data = inference_result['data']['result']\n",
    "                        result_data = inference_result['output']\n",
    "                        results.append(result_data)\n",
    "                        output_data.append([result_data])\n",
    "                        logger.info(f\"Questionnaire ID:, Result: {result_data}\")\n",
    "                        break\n",
    "                    except TypeError:\n",
    "                        print(\"发生错误：'NoneType'对象不支持索引操作\")\n",
    "                        results.append(\"错误：无效的响应\")\n",
    "                        output_data.append([\"错误：无效的响应\"])\n",
    "                        break\n",
    "\n",
    "                else:\n",
    "                    print(\"An error occurred: Request failed or timed out\")\n",
    "                    retries += 1\n",
    "                    time.sleep(5)  # 等待一秒后重试\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"处理请求时发生异常: {e}\")\n",
    "                logger.error(f\"SSL Error occurred 被抓到了: {e}\")\n",
    "                retries += 1\n",
    "                time.sleep(5)\n",
    "\n",
    "            if retries >= max_retries:\n",
    "                print(\"Max retries reached. Skipping this entry.\")\n",
    "                results.append(\"Error: Unable to process\")\n",
    "                output_data.append([\"Error: Unable to process\"])\n",
    "                break\n",
    "\n",
    "    # average_duration = sum(durations) / len(durations)\n",
    "    # print(f\"Average Response Time: {average_duration:.2f} seconds\")\n",
    "\n",
    "    with open(result_file, 'w', encoding='utf-8') as f:\n",
    "        for result in results:\n",
    "            f.write(result + '\\n')\n",
    "\n",
    "    output_df = pd.DataFrame(output_data, columns=['结果'])\n",
    "    output_df.to_excel(output_excel_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdb5e8-6788-4932-8e33-468a08b1fc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from requests.exceptions import SSLError\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6df74c3-3293-4af6-9eac-617f7d00be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 7小时后运行的延时，单位为秒\n",
    "    # delay = 7 * 60 * 60\n",
    "\n",
    "    # 使用time.sleep()进行延时\n",
    "    # time.sleep(delay)\n",
    "\n",
    "    # keywords = [\"chongtu\", \"gongtong\", \"guanxi\", \"xingyu\", \"yuanjing\"]  # 存储多个keyword\n",
    "    keywords = [\"topic\"]\n",
    "\n",
    "    file_path = \"result1/posts.txt\"\n",
    "    app_id = \"1681124596859981898\"  # 这里需要替换为实际的AppId\n",
    "\n",
    "    for keyword in keywords:\n",
    "        result_file = f\"result1/topic_result.txt\"\n",
    "        prompt_file_path = f\"prompt/fg_{keyword}_prompt_v2.txt\"\n",
    "        output_excel_path = f\"result1/fg_{keyword}_output.xlsx\"\n",
    "        log_file_path = f\"log/fg_{keyword}_result.log\"\n",
    "\n",
    "        # 为每个关键词创建一个新的Logger\n",
    "\n",
    "        logger = logging.getLogger(keyword)\n",
    "        logger.setLevel(logging.INFO)\n",
    "        # 避免重复日志\n",
    "        if not logger.handlers:\n",
    "            # 创建一个FileHandler用于写入日志文件\n",
    "            file_handler = RotatingFileHandler(log_file_path, maxBytes=1024 * 1024 * 100, backupCount=5,\n",
    "                                               encoding='utf-8')\n",
    "            file_handler.setFormatter(logging.Formatter('%(asctime)s - %(message)s'))\n",
    "            logger.addHandler(file_handler)\n",
    "\n",
    "        # 使用logger.info等方法记录日志\n",
    "        # logger.info(\"Your log message\")\n",
    "\n",
    "        process_inference(file_path, app_id, result_file, prompt_file_path, output_excel_path, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a0da67-c015-449f-8100-33bb9e32f976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载CSV文件\n",
    "df_csv = pd.read_csv('result1/2024_results.csv')\n",
    "\n",
    "# 读取txt文件，假设每行是一个结果，对应CSV文件的每行\n",
    "with open('result1/topic_result.txt', 'r', encoding='utf-8') as file:\n",
    "    topic_results = file.readlines()\n",
    "\n",
    "# 去除结果中的换行符\n",
    "topic_results = [result.strip() for result in topic_results]\n",
    "\n",
    "# 检查是否每行都有对应的结果\n",
    "if len(topic_results) != len(df_csv):\n",
    "    print(\"警告: txt文件中的结果数量与CSV文件中的行数不匹配。\")\n",
    "else:\n",
    "    # 将结果作为新列添加到DataFrame中\n",
    "    df_csv['result'] = topic_results\n",
    "\n",
    "    # 保存修改后的DataFrame到新的CSV文件\n",
    "    df_csv.to_csv('result1/2024_result_with_topic.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b161fc7-cae2-4202-9284-9e8814447713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 首先，过滤掉'df_csv'中'result'列值为'Error: Unable to process'的行\n",
    "df_filtered = df_csv[df_csv['result'] != 'Error: Unable to process']\n",
    "\n",
    "# 然后，将过滤后的DataFrame导出到csv文件\n",
    "df_filtered.to_csv('result1/2024_result_with_topic.csv', index=False)\n",
    "# df_filtered.to_excel('result/2024_result_with_topic.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a718cc0-ba04-4987-b25c-942795ed7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "# 加载csv文件\n",
    "df = pd.read_csv('result1/2024_result_with_topic.csv')\n",
    "\n",
    "# 定义一个辅助函数用于提取\"帖子类别\"和直接提取\"实体\"到\"}\"之间的所有内容\n",
    "def extract_category_and_entities(x):\n",
    "    category_pattern = r'\"帖子类别\":\\s*\"([^\"]+)\"'  # 匹配\"帖子类别\": \"xxx\"\n",
    "    entities_pattern = r'\"实体\":\\s*\"([^}]+)\"'  # 匹配\"实体\": \"xxx\"到\"}\"之间的内容\n",
    "\n",
    "    category_match = re.search(category_pattern, x)\n",
    "    entities_match = re.search(entities_pattern, x)\n",
    "\n",
    "    category = category_match.group(1) if category_match else None\n",
    "    entities = entities_match.group(1) if entities_match else None\n",
    "    # 如果存在多个实体，它们将通过逗号分隔，这里将它们分割成列表\n",
    "    entities_list = entities.split('\",\"') if entities else None\n",
    "\n",
    "    return category, entities_list\n",
    "\n",
    "# 应用辅助函数，提取\"帖子类别\"和\"实体\"到\"}\"之间的所有内容\n",
    "df['帖子类别'], df['实体'] = zip(*df['result'].apply(extract_category_and_entities))\n",
    "\n",
    "# 保存到新的csv文件\n",
    "df.to_csv('result1/topic_result_extracted_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65d062-bab7-4e3a-87c0-ff94f0f1973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载csv文件\n",
    "df = pd.read_csv('result1/topic_result_extracted_final.csv')\n",
    "\n",
    "# 对“帖子类别”字段进行预处理，去掉引号等符号但保留/\n",
    "df['帖子类别'] = df['帖子类别'].str.replace(r'[^\\w/]', '', regex=True)\n",
    "\n",
    "# 保存修改后的DataFrame到新的csv文件\n",
    "df.to_csv('result1/topic_result_preprocessed_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bb2ae9-7b25-437d-ae43-c0b7c8986d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载xlsx文件\n",
    "df = pd.read_csv('result1/topic_result_preprocessed_final.csv')\n",
    "\n",
    "\n",
    "# 定义映射表\n",
    "yst = {\n",
    "    \"客户服务\": \"用户体验\",\n",
    "    \"产品建议\": \"建议反馈\",\n",
    "    \"信息安全\": \"安全保障\",\n",
    "    \"技术问题\": \"技能发展\",\n",
    "    \"产品反馈\": \"建议反馈\",\n",
    "    \"薪酬管理\": \"薪酬福利\",\n",
    "    \"权限管理\": \"平台运营\",\n",
    "    \"业务发展\": \"市场竞争\",\n",
    "    \"业务合作\": \"平台合作\",\n",
    "    \"技术分享\": \"技术讲座\",\n",
    "    \"工作效率\": \"工作流程\",\n",
    "    \"员工关怀\": \"员工福利\",\n",
    "    \"工具使用\": \"办公设备\",\n",
    "    \"业务运营\": \"平台运营\",\n",
    "    \"技术讨论\": \"技能发展\",\n",
    "    \"产品功能\": \"功能使用\",\n",
    "    \"差旅管理\": \"考勤管理\"\n",
    "}\n",
    "\n",
    "# 假设xlsx文件中需要映射的列名为\"主题\"\n",
    "df['帖子类别'] = df['帖子类别'].map(yst).fillna(df['帖子类别'])\n",
    "\n",
    "# 替换DataFrame中\"帖子类别\"列中所有的\"为''\n",
    "df['帖子类别'] = df['帖子类别'].str.replace('\"', '')\n",
    "\n",
    "# 保存修改后的DataFrame到新的xlsx文件\n",
    "df.to_csv('result1/topic_result_mapped_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a455063-4f2e-45b1-bf14-2970da3cfcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 加载csv文件\n",
    "df = pd.read_csv('result1/topic_result_mapped_final.csv')\n",
    "\n",
    "# 选择posts_id和帖子类别列\n",
    "category_df = df[['posts_id', '帖子类别']]\n",
    "\n",
    "# 保存到新的csv文件\n",
    "category_df.to_csv('result1/category.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2076c8-e776-4cf1-ba2a-3ce48d65875b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 读取Excel文件\n",
    "df = pd.read_csv('result1/topic_result_mapped_final.csv')\n",
    "\n",
    "# 将“实体”列中所有数据先转换为字符串\n",
    "df['实体'] = df['实体'].astype(str)\n",
    "\n",
    "# 去掉“实体”列中的中括号和引号\n",
    "##df['实体'] = df['实体'].apply(lambda x: re.sub(r'[\\'\\[\\]]', '', x))\n",
    "df['实体'] = df['实体'].apply(lambda x: re.sub(r'[\\'\\[\\]\":]', '', x))\n",
    "\n",
    "# 将所有分隔符替换为英文逗号，包括中文逗号和中文顿号\n",
    "df['实体'] = df['实体'].apply(lambda x: re.sub(r'[，、]', ',', x))\n",
    "\n",
    "# 初次按逗号分割“实体”列，并展开为多行\n",
    "df_expanded = df.assign(实体=df['实体'].str.split(',')).explode('实体')\n",
    "\n",
    "# 去除拆分后的多余空格及任何符号\n",
    "df_expanded['实体'] = df_expanded['实体'].apply(lambda x: x.strip())\n",
    "\n",
    "# 对“实体”列再进行一次分割处理（如果仍然有其他分隔符存在）\n",
    "df_final = df_expanded.assign(实体=df_expanded['实体'].str.split(',')).explode('实体')\n",
    "\n",
    "# 处理后去除多余空格\n",
    "df_final['实体'] = df_final['实体'].apply(lambda x: x.strip())\n",
    "\n",
    "# 去除重复的实体\n",
    "df_final = df_final.drop_duplicates(subset=['posts_id', '实体'])\n",
    "\n",
    "# 只保留 posts_id 和 实体 两列\n",
    "df_to_save = df_final[['posts_id', '实体']]\n",
    "\n",
    "# 保存新的DataFrame到新的Excel文件\n",
    "df_to_save.to_csv('result1/new_topic_result_mapped_final.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024dee5e-bfc4-45de-a8ce-aadd60705e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# 假设你的Excel文件路径为'path_to_your_excel.xlsx'，entity列是你想要处理的列\n",
    "# df = pd.read_excel('../data/id_entity_transformed.xlsx')\n",
    "df = pd.read_csv('result1/new_topic_result_mapped_final.csv')\n",
    "\n",
    "retain_pattern = re.compile(r'L\\d+([-/]?\\d+)*([-/]?\\d+[A-Za-z]?)*')\n",
    "\n",
    "\n",
    "# 函数：删除非汉字和字母字符，但保留规定的格式\n",
    "def clean_entity(entity):\n",
    "    # 确保entity是字符串\n",
    "    if not isinstance(entity, str):\n",
    "        entity = str(entity)\n",
    "    # 去除空格\n",
    "    entity = entity.replace(' ', '')\n",
    "    # 检查是否匹配保留的格式\n",
    "    if retain_pattern.search(entity):\n",
    "        return entity\n",
    "    else:\n",
    "        # 删除非汉字和字母字符\n",
    "        cleaned_entity = re.sub(r'[^\\u4e00-\\u9fa5a-zA-Z]', '', entity)\n",
    "        # 剔除长度小于2的值\n",
    "        if len(cleaned_entity) < 2:\n",
    "            return None\n",
    "        return cleaned_entity\n",
    "\n",
    "\n",
    "# 对entity列进行处理\n",
    "df['实体'] = df['实体'].apply(clean_entity)\n",
    "\n",
    "# 删除值为None的行\n",
    "df = df.dropna(subset=['实体'])\n",
    "\n",
    "\n",
    "# 保存到新的Excel文件\n",
    "df.to_csv('result1/entry.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924dbd71-570c-4012-8d32-c15641614a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import re\n",
    "import csv\n",
    "\n",
    "#把实体变成规范写法\n",
    "# 规范的专业术语列表\n",
    "standard_terms = [\n",
    "    \"简称\",\n",
    "    \"AI\",\n",
    "    \"AIGC\",\n",
    "    \"AM\",\n",
    "    \"API\",\n",
    "    \"App\",\n",
    "    \"ARPU\",\n",
    "    \"BA\",\n",
    "    \"BA Copilot\",\n",
    "    \"Bar Raiser\",\n",
    "    \"BD\",\n",
    "    \"BDM\",\n",
    "    \"Better Box\",\n",
    "    \"BG\",\n",
    "    \"BG HR head\",\n",
    "    \"BGFP\",\n",
    "    \"BGM\",\n",
    "    \"BGOD\",\n",
    "    \"BM\",\n",
    "    \"BP\",\n",
    "    \"BP leader\",\n",
    "    \"BR\",\n",
    "    \"B-team\",\n",
    "    \"BU\",\n",
    "    \"bug\",\n",
    "    \"B端\",\n",
    "    \"C&B\",\n",
    "    \"C2\",\n",
    "    \"CAC\",\n",
    "    \"Calibri\",\n",
    "    \"CAP\",\n",
    "    \"CC\",\n",
    "    \"CKA\",\n",
    "    \"CLC\",\n",
    "    \"CMS\",\n",
    "    \"COD\",\n",
    "    \"CoE\",\n",
    "    \"Copilot\",\n",
    "    \"Correction of Error Report\",\n",
    "    \"CPA\",\n",
    "    \"CPC\",\n",
    "    \"CPT\",\n",
    "    \"CR\",\n",
    "    \"CRM\",\n",
    "    \"C端\",\n",
    "    \"DAU\",\n",
    "    \"DD\",\n",
    "    \"Debrief\",\n",
    "    \"EAP\",\n",
    "    \"EFM\",\n",
    "    \"EM\",\n",
    "    \"Empty Chair\",\n",
    "    \"ER\",\n",
    "    \"FAQ\",\n",
    "    \"final\",\n",
    "    \"Fire\",\n",
    "    \"Flywheel\",\n",
    "    \"FM\",\n",
    "    \"Forte\",\n",
    "    \"FP\",\n",
    "    \"GA\",\n",
    "    \"GFM\",\n",
    "    \"GMV\",\n",
    "    \"GROW\",\n",
    "    \"G-team\",\n",
    "    \"H1\",\n",
    "    \"H2\",\n",
    "    \"HC\",\n",
    "    \"HCM\",\n",
    "    \"HCP系统\",\n",
    "    \"HC系统\",\n",
    "    \"Hire\",\n",
    "    \"HRBA Copilot\",\n",
    "    \"HRBP\",\n",
    "    \"HRD\",\n",
    "    \"HRIS\",\n",
    "    \"HRM\",\n",
    "    \"HV\",\n",
    "    \"IC\",\n",
    "    \"IDP\",\n",
    "    \"Input\",\n",
    "    \"IPH\",\n",
    "    \"JD\",\n",
    "    \"Job\",\n",
    "    \"Job Family\",\n",
    "    \"Job Group\",\n",
    "    \"KA\",\n",
    "    \"KPA\",\n",
    "    \"KPI\",\n",
    "    \"LBS\",\n",
    "    \"Location\",\n",
    "    \"long-term\",\n",
    "    \"LP\",\n",
    "    \"LRP\",\n",
    "    \"LTI\",\n",
    "    \"LTV\",\n",
    "    \"MAU\",\n",
    "    \"MBR\",\n",
    "    \"MECE\",\n",
    "    \"MIS\",\n",
    "    \"MM\",\n",
    "    \"MO\",\n",
    "    \"MoM\",\n",
    "    \"NSP\",\n",
    "    \"OA\",\n",
    "    \"OC\",\n",
    "    \"OD\",\n",
    "    \"Offsite Meeting\",\n",
    "    \"OKR\",\n",
    "    \"OLR\",\n",
    "    \"OP\",\n",
    "    \"OP1\",\n",
    "    \"OP2\",\n",
    "    \"OPR\",\n",
    "    \"Org Review\",\n",
    "    \"output\",\n",
    "    \"OV\",\n",
    "    \"PA\",\n",
    "    \"Pay mix\",\n",
    "    \"Payroll\",\n",
    "    \"PDCA\",\n",
    "    \"PECL\",\n",
    "    \"PEST\",\n",
    "    \"PFM\",\n",
    "    \"Phasing\",\n",
    "    \"PIP\",\n",
    "    \"PM\",\n",
    "    \"PMC\",\n",
    "    \"POI\",\n",
    "    \"pp\",\n",
    "    \"PR\",\n",
    "    \"PrOACT\",\n",
    "    \"PV\",\n",
    "    \"PXT\",\n",
    "    \"QBR\",\n",
    "    \"QoQ\",\n",
    "    \"Range\",\n",
    "    \"RASCI\",\n",
    "    \"Review\",\n",
    "    \"ROI\",\n",
    "    \"Role Guideline\",\n",
    "    \"SA\",\n",
    "    \"SCQA\",\n",
    "    \"SDE\",\n",
    "    \"SF\",\n",
    "    \"SKU\",\n",
    "    \"SMART\",\n",
    "    \"SOA\",\n",
    "    \"SSC\",\n",
    "    \"STAR\",\n",
    "    \"S-Team goal\",\n",
    "    \"STL\",\n",
    "    \"SVP\",\n",
    "    \"SWOT\",\n",
    "    \"Tag\",\n",
    "    \"Talent Discussion\",\n",
    "    \"TB\",\n",
    "    \"TD\",\n",
    "    \"Team Goals\",\n",
    "    \"Tenets\",\n",
    "    \"The Andon Cord\",\n",
    "    \"TPA\",\n",
    "    \"TPM\",\n",
    "    \"TPU\",\n",
    "    \"TTC\",\n",
    "    \"TTP\",\n",
    "    \"Two-pizza Team\",\n",
    "    \"UV\",\n",
    "    \"VD\",\n",
    "    \"VOC\",\n",
    "    \"WB\",\n",
    "    \"WBR\",\n",
    "    \"Why\",\n",
    "    \"WoW\",\n",
    "    \"YoY\"\n",
    "]\n",
    "\n",
    "# 创建一个空字典用于存储映射关系\n",
    "term_mapping = {}\n",
    "for term in standard_terms:\n",
    "    term_mapping[term.lower()] = term\n",
    "\n",
    "# 定义一个函数，用于处理包含专业术语的字符串\n",
    "def standardize_term_in_context(non_standard_term):\n",
    "    changed = False  # 新增变量，用于标记是否进行了转换\n",
    "    for key, value in term_mapping.items():\n",
    "        pattern = re.compile(re.escape(key), re.IGNORECASE)\n",
    "        # 修改匿名函数，增加转换标记\n",
    "        non_standard_term, n = pattern.subn(lambda m: value, non_standard_term)\n",
    "        if n > 0:  # 如果发生了替换\n",
    "            changed = True\n",
    "    return non_standard_term, changed  # 返回转换后的字符串和是否转换的标记\n",
    "\n",
    "with open('result1/entry.csv', mode='r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    rows = list(reader)  # 将CSV文件的内容读取到列表中\n",
    "\n",
    "entity_column_index = None\n",
    "header = rows[0]  # CSV文件的标题行\n",
    "for index, title in enumerate(header):\n",
    "    if title == \"实体\":\n",
    "        entity_column_index = index\n",
    "        break\n",
    "\n",
    "if entity_column_index is not None:\n",
    "    for row_index, row in enumerate(rows):\n",
    "        if row_index == 0:  # 跳过标题行\n",
    "            continue\n",
    "        non_standard_term = row[entity_column_index]\n",
    "        if non_standard_term:\n",
    "            standardized_term, changed = standardize_term_in_context(non_standard_term)\n",
    "            if changed:\n",
    "                print(f\"行{row_index + 1}: '{non_standard_term}' 被转换为 '{standardized_term}'\")\n",
    "            row[entity_column_index] = standardized_term\n",
    "\n",
    "    # 将处理后的内容写回CSV文件\n",
    "    with open('result1/entry_guifan.csv', mode='w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(rows)\n",
    "else:\n",
    "    print(\"未找到名为'实体'的列\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
